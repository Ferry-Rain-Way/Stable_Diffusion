# Stable_Diffusion
## Lora 训练笔记
**训练方式**

Dreambooth无Caption：需要正则化，学习提示词+类型。这种就属于老的DB style训练，通过文件夹名字选择单一提示词触发。

Dreambooth带Caption：可以使用正则化，学习Caption或者标签。这是现在默认使用的训练方法，对每张图片对应的标签单独训练。这里的常见误解是文件夹名字对训练结果有有影响，其实有caption的时候文件夹名称仅仅用于控制重复次数。

Finetune相关技术：通过对预训练好的大模型使用少量样本进行finetune，使模型可以根据用户自定义的提示词生成个性化的图像，同时还基本不会影响模型在其他方面原本生成能力。

**数据集的选择、标注、处理**

首先要明确的是最终模型的质量取决于多方面的因素影响：数据集（标注），模型选择，参数选择等。但是不管训练怎样的模型，选择任何参数，数据集的影响应该是最优先考虑的。数据集的质量很大程度上决定了模型质量的上限，因为多数情况下数据都是木桶里最短的那一块木板。

首先，训练角色需要至少50张高质量的图片，画法需要至少100张，概念比较难以量化，通常来说50张是最低限度。这里并不是说少于这个数就没法训练出来，只是调参的过程会比较痛苦同时对高质量的标签依赖非常高。对于角色而言，尽量选择单人的图片（最多不要超过3人），尽量选择特征正确的图，同时画风尽量多样化以防止将画法烘焙进模型。同时可以加入不同服饰、大头照、NSFW的图片，就算你不打算生成这些内容，加入也可以使模型更好地捕获角色的特征。对于画风，尽量选择一致的画风。加入纯背景图片也可以增强模型泛化能力。 对于概念，同样需要画风多样化，同时需要大量的训练图片。

**分辨率和长宽比**

分辨率可以不为正方形，也可以不统一，因为脚本默认启用了Aspect Ratio Bucketing，会把相近长宽比的图放进一个batch里训练。
选择图像的分辨率不宜过小，如果需要可以使用RealESR-GAN或者CU-GAN这类算法上采样后学习，注意放大倍率不宜超过2，否则上采样后涂抹非常严重。大的分辨率并不影响学习过程，因为脚本会调用PIL库来缩放到训练大小，但是图太多一开始缓存latent的时候会比较慢。长宽比超过1：2的图片建议裁剪留下主要部分或者直接剔除，否则缩放后短边太短不利于学习。

**平衡训练集**

如果训练多个角色或者概念，确保每个角色的总图像量大致相同，否则大概率会生成多数样本的特征。平衡的方法是将含有该少数tag的图片单独放在一个子文件夹下，增加重复次数次数。

**标签删除**

一般来说可以删除你想要固化的属性，或者学习进触发词里面的属性，这样触发比较稳定并且简单。也可以选择不删除，这样训练出来的模型泛化性更好，人物的各种装扮可以自由组合。

**自动打标**

WD1.4 Tagger的自动打标效果还是非常不错的。这里建议是可以使用多个Tagger反复打标再删除重复的，这样可以减少漏打的概率。降低阈值也可以减少漏打的概率，但是同时错打的概率会变大。比较方便的方法是尽量减少漏打然后再手动删除掉错打的tag。

**标签合并**

将相似的tag合并为一个长的短语可以有效降低各个部分的颜色融合问题， 同时也可以缓解多标签带来的触发不稳定的情况。就算NAI的模型是在tag的基础上训练的，positional enconding也会增加相近token的关联性。

**避免Token冲突**

如果需要训练的内容大模型里面已经大量训练（>10K样本），如果你不想和大模型的内容发生融合，可以自己为特征取一个独特的名字。注意仅在样本数量大的时候可以这样做，否则模型容易学不会。

---

## 训练参数

**训练分辨率**

训练网络时使用的分辨率为（宽，高）,必须是64的整数倍。建议使用大于512x512且小于1024x1024的值，长宽比根据训练集的占比决定，一般来说方形的可以照顾到各种不同的分辨率。如果多数为长图可以使用512x768这种分辨率，如果宽图居多则可以使用768x512等。

**网络结构（LoRA/LoCon/LoHa）**

这些都是不同的矩阵低秩分解方法，LoRa只控制模型中的线性层，LoCon加入了对卷积层的控制，LoHa和LoKr分别使用两个矩阵的Hadamard积和Kronecker积来分解，能以同样参数大小达到同等秩（同等学习能力）。IA3和DyLoRA是刚刚Lycoris module更新的两种新方法，没有测试，故不作讨论。理论上来说同等参数下对模型的控制：LoRA < LoCon < LoHa同时各个方法对于画风的控制也是依次递增。训练人物建议使用LoRA或者LoCon,训练画风时建议使用LoHa。

**底模选择**

底模选择部分很大参考了huggingface上的实验以及Rentry上的总结。
训练人物使用NAI原版模型和ACertainty，训练画风使用Anylora。

因为现有的模型基本都来源于NAI和AC，所以在这上面训练的模型兼容性最好。使用Anylora可以做到在不同模型上生成保持画风。同时在相近的模型上训练的生成效果也会比较好。PastelMix和AOM等模型不合适训练，训练出来基本只能在十分接近这些模型的模型上使用。

---

## 总学习步数 = （图片数量 * 重复次数 * epoch）/ 批次大小

首先以UNet学习率为1e-4为例，一般来说图片较少的时候训练人物需要至少1000步，训练画风则需要至少2500步，训练概念则需要至少3000步。这里只是最低的步数，图片多则需要更多步数。学习率更大可以适当减少步数，但并非线性关系，使用两倍的学习率需要使用比之前步数的一半更多的步数。
决定学习率和步数的最好方法是先训练，再测试。一般比较好的初始值为UNet使用1e-4，TE使用5e-5。关于调整学习率有很多技巧，会在下一篇文章中集中讨论。

**学习率调整策略（lr_scheduler）**

推荐使用余弦退火cosine。如果开启预热，预热步数应该占总步数的5%-10%。
如果使用带重启的余弦退火cosine_with_restarts，重启次数不应该超过4次。

**批次大小 （batch_size）**
Batch size越大梯度越稳定，也可以使用更大的学习率来加速收敛，但是占用显存也更大。一般而言2倍的batch_size可以使用两倍的UNet学习率，但是TE学习率不能提高太多。

---

**分享社区**

civitai：比较主流的AI绘画模型分享网站。可以看到大佬们跑出的图，并复现别人的tag或者下载别人finetune的模型。使用webui的时候这个网站用起来比较方便。

Huggingface：一个数据集、模型分享的网站，很多业界大牛也在使用和提交新模型，我们可以从stable diffusion分区下载一些作者训练的模型。个人感觉使用diffusers的时候这个网站比较方便。个人常用的模型是"runwayml/stable-diffusion-v1-5"和"Linaqruf/anything-v3.0"（类似NovelAI的leak模型）。

---
**手脚等人物精细模型处理**
1.在负面提示词中添加残缺六指等负面提示词，但是修复效果不明显
2.在Controlent加载图片使用Openpose获取同款手指姿势，预处理器选择dw，但放大后还是有可能残留问题，进一步安装openpose-editor固定种子数可直接进入图片编辑调整手指状态后，发送至controlnet再次生成
3.画面中有两只以上的手补模型需要开启第二个controlnet控制单元加载原图选择完美像素模式和Depth，将控制权中降低至0.4-0.6，再将原openopose权重调整至相同的权重，再增加第三个controlnet控制单元加载原图选用soft edge，再次生成图片可完美解决
4.修改已生成好的样片因为骨骼图已确定的情况下无法直接使用Controlent，选择插件Depth Library，将默认手部模型放在stablediffusionwebui/extensions/sd-webui-depth-lib/maps路径下。
